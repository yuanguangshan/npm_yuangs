import { AgentInput, AgentMode } from './types';
import { ContextBuffer } from '../commands/contextBuffer';
import { inferIntent } from './intent';
import { buildContext } from './context';
import { buildPrompt } from './prompt';
import { selectModel } from './selectModel';
import { runLLM } from './llm';
import { interpretResultToPlan } from './interpret';
import { executePlan } from './planExecutor';
import { saveRecord } from './record';
import { learnSkillFromRecord } from './skills';
import { randomUUID } from 'crypto';
import { StreamMarkdownRenderer } from '../utils/renderer';
import ora, { Ora } from 'ora';
import chalk from 'chalk';

import { DefaultTokenPolicy } from '../policy/token/DefaultTokenPolicy';
import { ModelRegistry } from '../policy/model/ModelRegistry';
import { SyntaxHandler } from '../policy/syntaxHandler';
import { PolicyPresenter } from '../ui/PolicyPresenter';
import { UserDecision } from '../policy/token/types';
import { ContextSampler } from '../policy/sampler';

const MAX_PIPELINE_ITERATIONS = 3;

export class AgentPipeline {
    private contextBuffer: ContextBuffer = new ContextBuffer();
    private modelRegistry: ModelRegistry;
    private policy: DefaultTokenPolicy;

    constructor(modelRegistry?: ModelRegistry) {
        this.modelRegistry = modelRegistry || new ModelRegistry([]);
        this.policy = new DefaultTokenPolicy();
    }

    async run(input: AgentInput, mode: AgentMode): Promise<void> {
        const id = randomUUID();

        try {
            await this.runWithTokenPolicy(input, mode, id);
        } catch (error: any) {
            if (error.name === 'MaxIterationsExceeded') {
                console.log(chalk.yellow('\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ“ä½œç»ˆæ­¢'));
            } else {
                console.log(chalk.red(`\nâŒ Pipeline é”™è¯¯: ${error.message}`));
            }
        }

        PolicyPresenter.clearSuppressCache();
    }

    /**
     * æ‰§è¡Œå¸¦ TokenPolicy çš„ pipeline
     */
    private async runWithTokenPolicy(
        input: AgentInput,
        mode: AgentMode,
        executionId: string
    ): Promise<void> {
        // 1. æ„å›¾è§£æ (Syntax Phase)
        const tokens = this.extractContextTokens(input.rawInput);
        let pendingItems = SyntaxHandler.parse(tokens);

        // 2. æ²»ç†å®¡è®¡å¾ªç¯ (Governance Loop)
        // æœ€å¤šé‡è¯• 3 æ¬¡ï¼ˆåŒ…æ‹¬åˆå§‹è¯„ä¼°ï¼‰ä»¥é˜²æ­¢æ— é™å¾ªç¯
        const MAX_ITERATIONS = 3;
        let passed = false;
        let iterations = 0;
        let currentModel = this.modelRegistry.get(
            input.options?.model || 'gemini-2.5-flash-lite'
        ) || this.modelRegistry.getDefault();

        while (!passed && iterations < MAX_ITERATIONS) {
            iterations++;

            const result = await this.policy.evaluate({
                model: currentModel,
                contextItems: pendingItems,
                mode: this.determineMode(mode),
                userIntent: input.rawInput
            });

            passed = await this.handlePolicyResult(
                result,
                pendingItems,
                currentModel,
                iterations
            );

            if (passed) break;
        }

        if (!passed) {
            throw new Error('MaxIterationsExceeded');
        }

        // 3. æˆæƒæ‰§è¡Œ (Execution Phase)
        const resolved = await Promise.all(pendingItems.map(item => item.resolve()));

        resolved.forEach(r => {
            this.contextBuffer.add(
                {
                    type: 'file',
                    path: pendingItems.find((p: any) => p.id.includes(r.content.substring(0, 20)))?.id || 'unknown',
                    content: r.content
                },
                true // bypassTokenLimit = true (å·²é€šè¿‡ policy å®¡è®¡ï¼‰
            );
        });

        // 4. æ­£å¸¸ LLM Pipeline
        await this.executeLLMPipeline(input, mode, currentModel, executionId);
    }

    /**
     * å¤„ç† Policy ç»“æœ
     */
    private async handlePolicyResult(
        result: any,
        pendingItems: any[],
        currentModel: any,
        iteration: number
    ): Promise<boolean> {
        if (result.status === 'ok') {
            return true; // passed
        }

        if (result.status === 'block') {
            await PolicyPresenter.presentBlock(result);
            return false;
        }

        if (result.status === 'warn') {
            const decision = await PolicyPresenter.presentWarning(
                result,
                `${currentModel.name}:${pendingItems.map(p => p.id).join(',')}`
            );

            return this.applyDecision(decision, pendingItems, currentModel);
        }

        return false;
    }

    /**
     * åº”ç”¨ç”¨æˆ·å†³ç­–
     */
    private async applyDecision(
        decision: UserDecision,
        pendingItems: any[],
        currentModel: any
    ): Promise<boolean> {
        switch (decision.type) {
            case 'continue':
                return true;

            case 'abort':
                return false;

            case 'switch_model':
                if (decision.targetModel) {
                    const newModel = this.modelRegistry.get(decision.targetModel);
                    if (newModel) {
                        console.log(chalk.green(`\nğŸ”„ åˆ‡æ¢è‡³æ¨¡å‹: ${decision.targetModel}`));
                        currentModel = newModel;
                        return false; // éœ€è¦é‡æ–°è¯„ä¼°
                    }
                }
                console.log(chalk.yellow(`âš ï¸  æ¨¡å‹ ${decision.targetModel} æœªæ‰¾åˆ°`));
                return false;

            case 'sample':
                if (decision.strategy === 'head_tail') {
                    console.log(chalk.cyan('\nâœ‚ åº”ç”¨ head_tail é‡‡æ ·...'));
                    pendingItems = await Promise.all(
                        pendingItems.map(item =>
                            ContextSampler.applySampling(item, 'head_tail')
                        )
                    );
                    return false; // éœ€è¦é‡æ–°è¯„ä¼°
                }
                return true;

            default:
                return true;
        }
    }

    /**
     * æ‰§è¡Œ LLM Pipelineï¼ˆåŸæœ‰çš„æµç¨‹ï¼‰
     */
    private async executeLLMPipeline(
        input: AgentInput,
        mode: AgentMode,
        model: any,
        executionId: string
    ): Promise<void> {
        const intent = inferIntent(input, mode);
        const context = buildContext(input, this.contextBuffer);
        const prompt = buildPrompt(intent, context, mode, input.rawInput);

        let renderer: StreamMarkdownRenderer | undefined;
        let spinner: Ora | undefined;

        if (mode === 'chat') {
            spinner = ora(chalk.cyan('Thinking...')).start();
            renderer = new StreamMarkdownRenderer(chalk.bold.blue('ğŸ¤– AI: '), spinner);
        }

        const result = await runLLM({
            prompt,
            model: model.name,
            stream: mode === 'chat',
            onChunk: mode === 'chat' && renderer
                ? (s) => renderer!.onChunk(s)
                : undefined,
        });

        if (mode === 'chat' && renderer) {
            renderer.finish();
        }

        const isStreaming = mode === 'chat';
        const plan = interpretResultToPlan(result, intent, mode, isStreaming);
        result.plan = plan;

        saveRecord({
            id: executionId,
            timestamp: Date.now(),
            mode,
            input,
            prompt,
            model: model.name,
            llmResult: result,
            action: plan.tasks[0]?.type === 'shell' ? {
                type: 'execute',
                command: plan.tasks[0].payload.command,
                risk: plan.tasks[0].payload.risk
            } : { type: 'print', content: result.rawText },
        });

        const summary = await executePlan(plan, input.options);

        learnSkillFromRecord({
            id: executionId,
            timestamp: Date.now(),
            mode,
            input,
            prompt,
            model: model.name,
            llmResult: result,
            action: plan.tasks[0]?.type === 'shell' ? {
                type: 'execute',
                command: plan.tasks[0].payload.command,
                risk: plan.tasks[0].payload.risk
            } : { type: 'print', content: result.rawText },
        }, summary.success);

        if (input.options?.verbose) {
            console.log(`\n${'-'.repeat(50)}`);
            console.log(`Execution ID: ${executionId}`);
            console.log(`Model: ${model.name}`);
            console.log(`Latency: ${result.latencyMs}ms`);
            if (result.tokens) {
                console.log(`Tokens: ${result.tokens.total}`);
            }
            console.log(`${'-'.repeat(50)}\n`);
        }
    }

    /**
     * ä»è¾“å…¥ä¸­æå–ä¸Šä¸‹æ–‡ tokens (@file, #dir)
     */
    private extractContextTokens(rawInput: string): string[] {
        return rawInput
            .split(' ')
            .filter(token => token.startsWith('@') || token.startsWith('#'));
    }

    /**
     * ç¡®å®š mode
     */
    private determineMode(mode: AgentMode): "command" | "pipe" | "agent" {
        // å°† Agent mode æ˜ å°„åˆ° Policy mode
        // chat â†’ agent, command â†’ command, command+exec â†’ command
        if (mode === 'chat') {
            return 'agent';
        }
        if (mode === 'command+exec') {
            return 'command';
        }
        return mode; // command é»˜è®¤ä¸º command
    }
}
